{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcecLaF8cb-2"
      },
      "source": [
        "Second Version that using CIFAR-100 to train a Resnet-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J_f6t8h8cjgX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uujw7asVcuED",
        "outputId": "abbb2fe1-316f-486f-e0d0-83171706377b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Environment Configurating...\n",
            "Use device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "print(\">>> Environment Configurating...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Use device: {device}\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_root_path = '/content/drive/MyDrive/Datasets/cifar100_data'\n",
        "\n",
        "if not os.path.exists(drive_root_path):\n",
        "    os.makedirs(drive_root_path)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7rxKRkZc1ST",
        "outputId": "f1a623bc-4bd6-47f4-b420-ec81263cea4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Loading CIFAR-100 dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\">>> Loading CIFAR-100 dataset...\")\n",
        "trainset = torchvision.datasets.CIFAR100(root=drive_root_path, train=True,\n",
        "                                         download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root=drive_root_path, train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = trainset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y7DSf1Vqc2aJ"
      },
      "outputs": [],
      "source": [
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1mUxp-Bc68J",
        "outputId": "4f7ce1de-c39e-4987-fe7a-8a81beb5b7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Building the ResNet-18 model (Modified for CIFAR-100)...\n"
          ]
        }
      ],
      "source": [
        "print(\">>> Building the ResNet-18 model (Modified for CIFAR-100)...\")\n",
        "\n",
        "def get_cifar_resnet18():\n",
        "    net1 = models.resnet18(weights=None)\n",
        "    net1.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    net1.maxpool = nn.Identity()\n",
        "    net1.fc = nn.Linear(net1.fc.in_features, 100)\n",
        "    return net1\n",
        "\n",
        "net1 = get_cifar_resnet18().to(device)\n",
        "\n",
        "optimizer = optim.SGD(net1.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "total_epochs = 100\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWckSyAWc9w9",
        "outputId": "1c2917ed-700a-4fe4-b91d-e07d29fa8333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Start training (Total Epochs: 100)...\n",
            "Strategy: Mixup Enabled | Optimizer: SGD | Scheduler: CosineAnnealing\n",
            "[Epoch 001] Initial Loss: 2.3236\n",
            "[Epoch 010/100] Loss: 2.1636 | LR: 0.036050\n",
            "[Epoch 020/100] Loss: 2.0820 | LR: 0.021896\n",
            "[Epoch 030/100] Loss: 1.7998 | LR: 0.010492\n",
            "[Epoch 040/100] Loss: 1.6527 | LR: 0.002956\n",
            "[Epoch 050/100] Loss: 1.6067 | LR: 0.000025\n",
            "[Epoch 060/100] Loss: 1.6467 | LR: 0.001985\n",
            "[Epoch 070/100] Loss: 1.7260 | LR: 0.008646\n",
            "[Epoch 080/100] Loss: 1.9693 | LR: 0.019355\n",
            "[Epoch 090/100] Loss: 2.0941 | LR: 0.033063\n",
            "[Epoch 100/100] Loss: 2.1906 | LR: 0.048429\n",
            "Training completed!\n",
            "The model has been saved to: /content/drive/MyDrive/Datasets/cifar100_data/cifar100_resnet18_scratch.pth\n"
          ]
        }
      ],
      "source": [
        "print(f\">>> Start training (Total Epochs: {total_epochs})...\")\n",
        "print(\"Strategy: Mixup Enabled | Optimizer: SGD | Scheduler: CosineAnnealing\")\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    net1.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=1.0)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net1(inputs)\n",
        "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    train_losses.append(avg_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'[Epoch {epoch+1:03d}/{total_epochs}] Loss: {avg_loss:.4f} | LR: {current_lr:.6f}')\n",
        "    elif (epoch + 1) == 1:\n",
        "        print(f'[Epoch 001] Initial Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"Training completed!\")\n",
        "torch.save(net1.state_dict(), f'{drive_root_path}/cifar100_resnet18_scratch.pth')\n",
        "print(f\"The model has been saved to: {drive_root_path}/cifar100_resnet18_scratch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3iVnn11dAZl",
        "outputId": "f4d37139-476d-474d-a3fa-2e1d66c10a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> The test set undergoing a comprehensive evaluation...\n",
            "\n",
            "========================================\n",
            "Evaluation Report (Global Metrics)\n",
            "========================================\n",
            "Accuracy  : 63.31%\n",
            "Weighted F1 : 62.71%\n",
            "\n",
            "----------------------------------------\n",
            "Target Category 'bus' Performance\n",
            "----------------------------------------\n",
            "Precision : 58.10%\n",
            "Recall    : 61.00%\n",
            "F1-Score  : 59.51%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_performance(model, dataloader, classes, target_class_name='bus'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    print(\">>> The test set undergoing a comprehensive evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Evaluation Report (Global Metrics)\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Accuracy  : {acc:.2%}\")\n",
        "\n",
        "    report_dict = classification_report(all_targets, all_preds, target_names=classes, output_dict=True, zero_division=0)\n",
        "    weighted = report_dict['weighted avg']\n",
        "    print(f\"Weighted F1 : {weighted['f1-score']:.2%}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(f\"Target Category '{target_class_name}' Performance\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "    if target_class_name in classes:\n",
        "        stats = report_dict[target_class_name]\n",
        "        print(f\"Precision : {stats['precision']:.2%}\")\n",
        "        print(f\"Recall    : {stats['recall']:.2%}\")\n",
        "        print(f\"F1-Score  : {stats['f1-score']:.2%}\")\n",
        "    else:\n",
        "        print(f\"Category not found: {target_class_name}\")\n",
        "\n",
        "evaluate_performance(net1, testloader, classes, target_class_name='bus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfL9ek0VPzyT"
      },
      "source": [
        "The New Version that use fine-tuning strategy on pretrained Resnet-18 instead of building & training from the ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdBRzZ4IRzX8",
        "outputId": "7dff916c-f325-44fc-cfb6-16a10d24b640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Setting up...\n",
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "zip detected: /content/drive/MyDrive/Datasets/cifar100_data/cifar-100-python.tar.gz\n",
            "Success!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\">>> Setting up...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_root_path = '/content/drive/MyDrive/Datasets/cifar100_data'\n",
        "\n",
        "if not os.path.exists(drive_root_path):\n",
        "    os.makedirs(drive_root_path)\n",
        "\n",
        "expected_file = os.path.join(drive_root_path, 'cifar-100-python.tar.gz')\n",
        "expected_folder = os.path.join(drive_root_path, 'cifar-100-python')\n",
        "\n",
        "if os.path.exists(expected_file):\n",
        "    print(f\"zip detected: {expected_file}\")\n",
        "elif os.path.exists(expected_folder):\n",
        "    print(f\"folder detected: {expected_folder}\")\n",
        "else:\n",
        "    print(f\"file not found. Current path: {drive_root_path}\")\n",
        "\n",
        "try:\n",
        "    trainset = torchvision.datasets.CIFAR100(root=drive_root_path, train=True, download=True)\n",
        "    print(\"Success!\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DyWSu0AMt7B",
        "outputId": "cff5add2-ea91-4100-9eec-3ab4c1d8944a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Loading CIFAR-100, resizing images to 224x224.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Downloading ImageNet pre-trained weights...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 239MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Start fine-tuning for 5 epochs...\n",
            "[Epoch 1, Batch 0] Loss: 5.081\n",
            "[Epoch 1, Batch 100] Loss: 4.260\n",
            "[Epoch 1, Batch 200] Loss: 3.481\n",
            "[Epoch 1, Batch 300] Loss: 2.721\n",
            "[Epoch 1, Batch 400] Loss: 2.472\n",
            "[Epoch 1, Batch 500] Loss: 1.947\n",
            "[Epoch 1, Batch 600] Loss: 2.230\n",
            "[Epoch 1, Batch 700] Loss: 1.763\n",
            "[Epoch 1, Batch 800] Loss: 1.363\n",
            "[Epoch 1, Batch 900] Loss: 1.151\n",
            "[Epoch 1, Batch 1000] Loss: 1.016\n",
            "[Epoch 1, Batch 1100] Loss: 1.142\n",
            "[Epoch 1, Batch 1200] Loss: 1.205\n",
            "[Epoch 1, Batch 1300] Loss: 1.062\n",
            "[Epoch 1, Batch 1400] Loss: 0.870\n",
            "[Epoch 1, Batch 1500] Loss: 0.935\n",
            "Epoch 1 Done. Avg Loss: 1.999\n",
            "[Epoch 2, Batch 0] Loss: 1.172\n",
            "[Epoch 2, Batch 100] Loss: 0.984\n",
            "[Epoch 2, Batch 200] Loss: 0.882\n",
            "[Epoch 2, Batch 300] Loss: 0.817\n",
            "[Epoch 2, Batch 400] Loss: 0.932\n",
            "[Epoch 2, Batch 500] Loss: 1.684\n",
            "[Epoch 2, Batch 600] Loss: 1.233\n",
            "[Epoch 2, Batch 700] Loss: 1.081\n",
            "[Epoch 2, Batch 800] Loss: 0.884\n",
            "[Epoch 2, Batch 900] Loss: 0.693\n",
            "[Epoch 2, Batch 1000] Loss: 0.799\n",
            "[Epoch 2, Batch 1100] Loss: 0.783\n",
            "[Epoch 2, Batch 1200] Loss: 1.193\n",
            "[Epoch 2, Batch 1300] Loss: 1.009\n",
            "[Epoch 2, Batch 1400] Loss: 0.735\n",
            "[Epoch 2, Batch 1500] Loss: 0.737\n",
            "Epoch 2 Done. Avg Loss: 0.907\n",
            "[Epoch 3, Batch 0] Loss: 0.789\n",
            "[Epoch 3, Batch 100] Loss: 0.503\n",
            "[Epoch 3, Batch 200] Loss: 0.741\n",
            "[Epoch 3, Batch 300] Loss: 1.003\n",
            "[Epoch 3, Batch 400] Loss: 0.679\n",
            "[Epoch 3, Batch 500] Loss: 0.679\n",
            "[Epoch 3, Batch 600] Loss: 0.749\n",
            "[Epoch 3, Batch 700] Loss: 0.887\n",
            "[Epoch 3, Batch 800] Loss: 0.390\n",
            "[Epoch 3, Batch 900] Loss: 0.848\n",
            "[Epoch 3, Batch 1000] Loss: 0.587\n",
            "[Epoch 3, Batch 1100] Loss: 0.493\n",
            "[Epoch 3, Batch 1200] Loss: 0.932\n",
            "[Epoch 3, Batch 1300] Loss: 0.835\n",
            "[Epoch 3, Batch 1400] Loss: 0.562\n",
            "[Epoch 3, Batch 1500] Loss: 0.838\n",
            "Epoch 3 Done. Avg Loss: 0.676\n",
            "[Epoch 4, Batch 0] Loss: 0.544\n",
            "[Epoch 4, Batch 100] Loss: 0.566\n",
            "[Epoch 4, Batch 200] Loss: 0.576\n",
            "[Epoch 4, Batch 300] Loss: 0.269\n",
            "[Epoch 4, Batch 400] Loss: 0.623\n",
            "[Epoch 4, Batch 500] Loss: 0.552\n",
            "[Epoch 4, Batch 600] Loss: 0.720\n",
            "[Epoch 4, Batch 700] Loss: 0.652\n",
            "[Epoch 4, Batch 800] Loss: 0.297\n",
            "[Epoch 4, Batch 900] Loss: 0.433\n",
            "[Epoch 4, Batch 1000] Loss: 0.428\n",
            "[Epoch 4, Batch 1100] Loss: 0.636\n",
            "[Epoch 4, Batch 1200] Loss: 0.525\n",
            "[Epoch 4, Batch 1300] Loss: 0.771\n",
            "[Epoch 4, Batch 1400] Loss: 0.494\n",
            "[Epoch 4, Batch 1500] Loss: 0.728\n",
            "Epoch 4 Done. Avg Loss: 0.535\n",
            "[Epoch 5, Batch 0] Loss: 0.253\n",
            "[Epoch 5, Batch 100] Loss: 0.221\n",
            "[Epoch 5, Batch 200] Loss: 0.453\n",
            "[Epoch 5, Batch 300] Loss: 0.478\n",
            "[Epoch 5, Batch 400] Loss: 0.207\n",
            "[Epoch 5, Batch 500] Loss: 0.470\n",
            "[Epoch 5, Batch 600] Loss: 0.541\n",
            "[Epoch 5, Batch 700] Loss: 0.363\n",
            "[Epoch 5, Batch 800] Loss: 0.209\n",
            "[Epoch 5, Batch 900] Loss: 0.706\n",
            "[Epoch 5, Batch 1000] Loss: 0.413\n",
            "[Epoch 5, Batch 1100] Loss: 0.460\n",
            "[Epoch 5, Batch 1200] Loss: 0.671\n",
            "[Epoch 5, Batch 1300] Loss: 0.577\n",
            "[Epoch 5, Batch 1400] Loss: 0.518\n",
            "[Epoch 5, Batch 1500] Loss: 0.512\n",
            "Epoch 5 Done. Avg Loss: 0.433\n",
            "All Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "transform_transfer = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\">>> Loading CIFAR-100, resizing images to 224x224.\")\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform_transfer)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform_transfer)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "print(\">>> Downloading ImageNet pre-trained weights...\")\n",
        "net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 100)\n",
        "net = net.to(device)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5\n",
        "print(f\">>> Start fine-tuning for {epochs} epochs...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'[Epoch {epoch+1}, Batch {i}] Loss: {loss.item():.3f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1} Done. Avg Loss: {running_loss / len(trainloader):.3f}')\n",
        "\n",
        "print(\"All Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l749s3eRnIr",
        "outputId": "773d4335-2e41-4456-bdf4-cdb0866ff752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Evaluating on CIFAR-100 test set...\n",
            ">>> This may take some time, processing 10,000 images of size 224x224\n",
            "\n",
            "==================================================\n",
            "Final Model Performance (ResNet-18 Fine-tuned)\n",
            "==================================================\n",
            "Overall Accuracy: 80.29%\n",
            "--------------------------------------------------\n",
            "Weighted Average Metrics:\n",
            "Precision: 80.83%\n",
            "Recall:    80.29%\n",
            "F1-Score:  80.24%\n",
            "--------------------------------------------------\n",
            "Target Category Performance (CAPTCHA Task Focus):\n",
            "Class Name      | Precision  | Recall     | F1-Score  \n",
            "-------------------------------------------------------\n",
            "bus             | 77.78%    | 84.00%    | 80.77%\n",
            "pickup_truck    | 92.08%    | 93.00%    | 92.54%\n",
            "train           | 83.96%    | 89.00%    | 86.41%\n",
            "truck           | Category not found\n",
            "streetcar       | 78.64%    | 81.00%    | 79.80%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8029"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "testset_eval = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                            download=True, transform=transform_eval)\n",
        "testloader_eval = torch.utils.data.DataLoader(testset_eval, batch_size=32,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "classes = testset_eval.classes\n",
        "\n",
        "def evaluate_resnet_performance(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    print(f\">>> Evaluating on CIFAR-100 test set...\")\n",
        "    print(f\">>> This may take some time, processing 10,000 images of size 224x224\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Final Model Performance (ResNet-18 Fine-tuned)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Overall Accuracy: {acc:.2%}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    report = classification_report(all_targets, all_preds, target_names=classes, output_dict=True)\n",
        "    weighted_avg = report['weighted avg']\n",
        "    print(f\"Weighted Average Metrics:\")\n",
        "    print(f\"Precision: {weighted_avg['precision']:.2%}\")\n",
        "    print(f\"Recall:    {weighted_avg['recall']:.2%}\")\n",
        "    print(f\"F1-Score:  {weighted_avg['f1-score']:.2%}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    target_vehicles = ['bus', 'pickup_truck', 'train', 'truck', 'streetcar']\n",
        "    print(\"Target Category Performance (CAPTCHA Task Focus):\")\n",
        "    print(f\"{'Class Name':<15} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    for class_name in target_vehicles:\n",
        "        if class_name in report:\n",
        "            stats = report[class_name]\n",
        "            print(f\"{class_name:<15} | {stats['precision']:.2%}    | {stats['recall']:.2%}    | {stats['f1-score']:.2%}\")\n",
        "        else:\n",
        "            print(f\"{class_name:<15} | Category not found\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "evaluate_resnet_performance(net, testloader_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el4osaMXh79I"
      },
      "source": [
        "Testing performace on self-prepared test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glbe_M9xh67v",
        "outputId": "beb5cd70-fa60-437d-a636-c557585d11db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (4.13.5)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from icrawler) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from icrawler) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2025.11.12)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
          ]
        }
      ],
      "source": [
        "!pip install icrawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OOk5edFiDWV",
        "outputId": "44b29a8a-6298-4be3-9c57-83091e292148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> [1/5] Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            ">>> [2/5] Getting CIFAR-100 class list...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:14<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Randomly selected 10 target classes: ['sunflower', 'trout', 'couch', 'maple_tree', 'mouse', 'lamp', 'rocket', 'mushroom', 'streetcar', 'dolphin']\n",
            "\n",
            "Crawling: sunflower ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 404, file https://i0.wp.com/dhcrop.bsmrau.net/wp-content/uploads/2018/01/Sunflower.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.dbrl.org/wp-content/uploads/2023/04/sunflower-5539198_1920.jpg, error: HTTPSConnectionPool(host='www.dbrl.org', port=443): Max retries exceeded with url: /wp-content/uploads/2023/04/sunflower-5539198_1920.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7829aec0c5f0>, 'Connection to www.dbrl.org timed out. (connect timeout=5)')), remaining retry times: 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: trout ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://png.pngtree.com/background/20230530/original/pngtree-brightly-colored-brown-trout-that-is-swimming-picture-image_2793164.jpg\n",
            "ERROR:downloader:Response status code 403, file https://png.pngtree.com/background/20230519/original/pngtree-brown-trout-swimming-underwater-under-rocks-picture-image_2654419.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.fishing.net.nz/sites/default/assets/Image/2022/spotting-brown-trout-2.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: couch ...\n",
            "\n",
            "Crawling: maple_tree ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 429, file https://peggywritesblog.files.wordpress.com/2018/06/maple-tree.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: mouse ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/14/f2/mouse_computer_input_device_hardware_pc_information_red-877317.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/computer-technology-white-mouse-isolated-product-multimedia-peripheral-input-device-electronic-device-computer-component-1090348.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: lamp ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.therange.co.uk/media/3/9/1661875385_7965.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.glow.co.uk/media/catalog/product/cache/1/image/960x/9df78eab33525d08d6e5fb8d27136e95/g/i/gingko-smart-galaxy-lamp-black-4.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: rocket ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://i.stack.imgur.com/4G4fd.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/73/bc/space_shuttle_lift_off_shuttle_space_launch_exploration_spaceship_sky-757839.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/5f/db/discovery_space_shuttle_rollout_launch_pad_pre_launch_astronaut_mission_exploration_spaceship-704300.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/35/d2/discovery_fire_galaxy_launch_liftoff_rocket_science_space-1116143.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/2f/b2/pakistan_missile_technology_weapon_deadly_missile_rocket_warfare_defence-499202.jpg\n",
            "ERROR:downloader:Response status code 403, file https://i.stack.imgur.com/MfqKu.jpg\n",
            "ERROR:downloader:Response status code 403, file https://i.stack.imgur.com/GwW4F.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: mushroom ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-flower-autumn-mushroom-close-forest-floor-fungus-mushrooms-toxic-agaric-bolete-fungal-species-forest-mushroom-lamellar-matsutake-matryoshka-forest-plant-symbol-of-good-luck-screen-fungus-edible-mushroom-medicinal-mushroom-agaricaceae-penny-bun-603884.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-meadow-prairie-flower-gift-red-autumn-mushroom-point-flora-fungus-spotted-fly-agaric-toxic-amanita-agaric-bolete-amanita-muscaria-muscaria-red-fly-agaric-mushroom-macro-photography-edible-mushroom-medicinal-mushroom-penny-bun-amanitaceae-1157240.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-flower-autumn-botany-mushroom-flora-fauna-fungus-mushrooms-woodland-agaric-bolete-agaricus-matsutake-oyster-mushroom-edible-mushroom-medicinal-mushroom-agaricomycetes-agaricaceae-champignon-mushroom-penny-bun-pleurotus-eryngii-449717.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/92/9b/forest_fungi_fungus_mushrooms_nature-1181460.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/c3/5c/fly_agaric_lucky_guy_amanita_muscaria_fungal_species_red_firs_mushroom_autumn-1215748.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/67/11/mushrooms_tree_fungus_forest_brown_lamellar_b_schelig_group_pile-850709.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-autumn-mushroom-fungus-fly-agaric-agaric-bolete-agaricus-edible-mushroom-medicinal-mushroom-agaricaceae-penny-bun-1261754.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.thedailyworld.com/wp-content/uploads/2024/06/36606047_web1_M1_ADW20240613_Muscaria-Mushroom-Gummies-Teaser.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-leaf-autumn-soil-mushroom-flora-season-fauna-leaves-fungus-mushrooms-woodland-agaric-bolete-agaricus-macro-photography-bovist-oyster-mushroom-edible-mushroom-medicinal-mushroom-agaricomycetes-agaricaceae-champignon-mushroom-penny-bun-1135290.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/eb/ae/mushroom_amanita_muscaria_fly_agaric_toadstool_cap_fungus_nature_fungi-1373582.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Crawling: streetcar ...\n",
            "\n",
            "Crawling: dolphin ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/2f/a4/ocean_nikon_dolphin_maldives_d4_nikond-412381.jpg\n",
            "ERROR:downloader:Response status code 404, file http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/A_spinner_dolphin_in_the_Red_Sea.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/sea-water-nature-ocean-play-wave-animal-wildlife-wild-swim-splash-biology-mammal-blue-aquatic-flipper-life-fin-waves-marine-mammals-vertebrate-dolphin-pacific-dolphins-intelligent-marine-mammal-delphinidae-marine-biology-spinner-dolphin-whales-dolphins-and-porpoises-stenella-common-bottlenose-dolphin-short-beaked-common-dolphin-rough-toothed-dolphin-striped-dolphin-tucuxi-617045.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/wildlife-biology-mammal-fauna-mountains-newzealand-waterfalls-vertebrate-dolphin-fiordlandnationialpark-dolphins-boatride-grandpacifictours-jucycruises-highcliffs-lunchcruise-marine-mammal-killer-whale-marine-biology-whales-dolphins-and-porpoises-common-bottlenose-dolphin-short-beaked-common-dolphin-rough-toothed-dolphin-179370.jpg\n",
            "ERROR:downloader:Response status code 403, file https://i2.pickpik.com/photos/94/337/114/dolphin-marine-mammals-animals-sea-thumb.jpg\n",
            "ERROR:downloader:Response status code 403, file https://i2.pickpik.com/photos/867/803/648/bottlenose-dolphin-marine-mammal-fish-marine-thumb.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> [4/5] Processing and converting images...\n",
            "\n",
            "Dataset creation completed!\n",
            "Storage location: /content/drive/MyDrive/Datasets/auto_test_set/clean_224\n",
            "Total valid images: 150\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "from google.colab import drive\n",
        "\n",
        "print(\">>> [1/5] Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_root = \"/content/drive/MyDrive/Datasets/auto_test_set\"\n",
        "raw_root = os.path.join(base_root, \"raw_temp\")\n",
        "final_root = os.path.join(base_root, \"clean_224\")\n",
        "\n",
        "if os.path.exists(base_root):\n",
        "    shutil.rmtree(base_root)\n",
        "os.makedirs(raw_root, exist_ok=True)\n",
        "os.makedirs(final_root, exist_ok=True)\n",
        "\n",
        "print(\">>> [2/5] Getting CIFAR-100 class list...\")\n",
        "temp_set = torchvision.datasets.CIFAR100(root='./temp_data', train=False, download=True)\n",
        "cifar100_classes = temp_set.classes\n",
        "\n",
        "selected_keywords = random.sample(cifar100_classes, 10)\n",
        "print(f\"Randomly selected 10 target classes: {selected_keywords}\")\n",
        "\n",
        "def download_images(keywords, max_num=20):\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\nCrawling: {keyword} ...\")\n",
        "        search_term = keyword + \" photo\"\n",
        "        save_dir = os.path.join(raw_root, keyword)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        crawler = BingImageCrawler(\n",
        "            downloader_threads=4,\n",
        "            storage={\"root_dir\": save_dir},\n",
        "            log_level=\"ERROR\"\n",
        "        )\n",
        "        crawler.crawl(keyword=search_term, max_num=max_num)\n",
        "\n",
        "def process_images_for_model():\n",
        "    print(\"\\n>>> [4/5] Processing and converting images...\")\n",
        "    valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "    count = 0\n",
        "\n",
        "    for class_name in os.listdir(raw_root):\n",
        "        class_dir = os.path.join(raw_root, class_name)\n",
        "        if not os.path.isdir(class_dir): continue\n",
        "\n",
        "        target_dir = os.path.join(final_root, class_name)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if not any(filename.lower().endswith(ext) for ext in valid_exts): continue\n",
        "\n",
        "            src_path = os.path.join(class_dir, filename)\n",
        "\n",
        "            try:\n",
        "                with Image.open(src_path) as img:\n",
        "                    img = img.convert(\"RGB\")\n",
        "                    img_resized = img.resize((224, 224), Image.BICUBIC)\n",
        "                    new_filename = f\"{class_name}_{count}.jpg\"\n",
        "                    img_resized.save(os.path.join(target_dir, new_filename), \"JPEG\", quality=95)\n",
        "                    count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping corrupted image: {filename}\")\n",
        "\n",
        "    print(f\"\\nDataset creation completed!\")\n",
        "    print(f\"Storage location: {final_root}\")\n",
        "    print(f\"Total valid images: {count}\")\n",
        "\n",
        "download_images(selected_keywords, max_num=15)\n",
        "process_images_for_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScSYW8y0oR-T",
        "outputId": "51b46f9c-7501-450f-e592-274bb73e94c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set built successfully: 150 images\n",
            "   (Automatically aligned with CIFAR-100 indices and applied ImageNet Normalization)\n",
            "DataLoader ready, can be passed directly to evaluation function!\n",
            "Batch Shape: torch.Size([32, 3, 224, 224])\n",
            "Batch Labels: tensor([82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 91, 91, 91,\n",
            "        91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 25, 25])\n",
            "Corresponding class names: ['sunflower', 'sunflower', 'sunflower', 'sunflower', 'sunflower']\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "class ExactIndexDataset(Dataset):\n",
        "    def __init__(self, root_dir, cifar_classes, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.cifar_classes = cifar_classes\n",
        "        self.samples = []\n",
        "\n",
        "        if not os.path.exists(root_dir):\n",
        "            raise RuntimeError(f\"Directory does not exist: {root_dir}\")\n",
        "\n",
        "        for class_name in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                if class_name in self.cifar_classes:\n",
        "                    real_index = self.cifar_classes.index(class_name)\n",
        "                    for img_name in os.listdir(class_path):\n",
        "                        if img_name.lower().endswith(('.jpg', '.jpeg')):\n",
        "                            self.samples.append((os.path.join(class_path, img_name), real_index))\n",
        "                else:\n",
        "                    print(f\"Warning: Folder '{class_name}' is not a CIFAR-100 class, ignored.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def get_verified_test_loader(data_root, batch_size=32):\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    temp_set = datasets.CIFAR100(root='./temp_data_verify', train=False, download=True)\n",
        "    full_classes = temp_set.classes\n",
        "\n",
        "    custom_dataset = ExactIndexDataset(\n",
        "        root_dir=data_root,\n",
        "        cifar_classes=full_classes,\n",
        "        transform=transform_eval\n",
        "    )\n",
        "\n",
        "    if len(custom_dataset) == 0:\n",
        "        print(\"Error: Dataset is empty! Please check path or folder names.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Test set built successfully: {len(custom_dataset)} images\")\n",
        "    print(f\"   (Automatically aligned with CIFAR-100 indices and applied ImageNet Normalization)\")\n",
        "\n",
        "    loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return loader, full_classes\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_path = \"/content/drive/MyDrive/Datasets/auto_test_set/clean_224\"\n",
        "    homemade_loader, classes_list = get_verified_test_loader(test_path)\n",
        "\n",
        "    if homemade_loader:\n",
        "        print(\"DataLoader ready, can be passed directly to evaluation function!\")\n",
        "\n",
        "        images, labels = next(iter(homemade_loader))\n",
        "        print(f\"Batch Shape: {images.shape}\")\n",
        "        print(f\"Batch Labels: {labels}\")\n",
        "        print(f\"Corresponding class names: {[classes_list[i] for i in labels[:5]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI5gFbeJqg0N",
        "outputId": "fde1d2ad-cace-4964-cbc5-70d8d704f031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Evaluating custom test set...\n",
            "\n",
            "==================================================\n",
            "Custom Test Set Performance\n",
            "==================================================\n",
            "Subset Accuracy: 55.33%\n",
            "--------------------------------------------------\n",
            "Class Name           | Precision  | Recall     | F1-Score   | Count\n",
            "-----------------------------------------------------------------\n",
            "couch                | 100.00%    | 93.33%    | 96.55%    | 15   \n",
            "dolphin              | 100.00%    | 53.33%    | 69.57%    | 15   \n",
            "lamp                 | 92.31%    | 80.00%    | 85.71%    | 15   \n",
            "maple_tree           | 0.00%    | 0.00%    | 0.00%    | 15   \n",
            "mouse                | 100.00%    | 20.00%    | 33.33%    | 15   \n",
            "mushroom             | 100.00%    | 73.33%    | 84.62%    | 15   \n",
            "rocket               | 100.00%    | 33.33%    | 50.00%    | 15   \n",
            "streetcar            | 100.00%    | 60.00%    | 75.00%    | 15   \n",
            "sunflower            | 83.33%    | 100.00%    | 90.91%    | 15   \n",
            "trout                | 100.00%    | 40.00%    | 57.14%    | 15   \n",
            "-----------------------------------------------------------------\n",
            "Weighted F1 (for current data only): 64.28%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_resnet_performance(model, dataloader, classes=None):\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    print(f\">>> Evaluating custom test set...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Custom Test Set Performance\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Subset Accuracy: {acc:.2%}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "    if classes is None:\n",
        "        print(\"Error: Must provide classes list to generate detailed report\")\n",
        "        return\n",
        "\n",
        "\n",
        "    all_indices = list(range(len(classes)))\n",
        "\n",
        "\n",
        "    report = classification_report(\n",
        "        all_targets,\n",
        "        all_preds,\n",
        "        labels=all_indices,\n",
        "        target_names=classes,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"{'Class Name':<20} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10} | {'Count':<5}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    found_any = False\n",
        "    for class_name in classes:\n",
        "        if class_name in report:\n",
        "            stats = report[class_name]\n",
        "            if stats['support'] > 0:\n",
        "                found_any = True\n",
        "                print(f\"{class_name:<20} | {stats['precision']:.2%}    | {stats['recall']:.2%}    | {stats['f1-score']:.2%}    | {int(stats['support']):<5}\")\n",
        "\n",
        "    if not found_any:\n",
        "        print(\"Warning: Report generated but no known classes found in data. Please check mapping logic.\")\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    weighted = report['weighted avg']\n",
        "    print(f\"Weighted F1 (for current data only): {weighted['f1-score']:.2%}\")\n",
        "evaluate_resnet_performance(net, homemade_loader, classes=classes_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnFIkutK1M-B"
      },
      "source": [
        "Dual-Channel Data-loding Pipeline  + Final Test Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpRxu-ZrszxX",
        "outputId": "a15d2321-0106-41a0-95ba-ef63813323d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.12/dist-packages (0.6.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (4.13.5)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.12/dist-packages (from icrawler) (0.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from icrawler) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from icrawler) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from icrawler) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->icrawler) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->icrawler) (2025.11.12)\n",
            ">>> [1/5] Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Environment ready. Working directory: /content/drive/MyDrive/Datasets/final_test_pipeline\n"
          ]
        }
      ],
      "source": [
        "!pip install icrawler\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import uuid\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\">>> [1/5] Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_ROOT = \"/content/drive/MyDrive/Datasets/final_test_pipeline\"\n",
        "RAW_ROOT = os.path.join(BASE_ROOT, \"raw_temp\")\n",
        "DIR_32   = os.path.join(BASE_ROOT, \"clean_32\")\n",
        "DIR_224  = os.path.join(BASE_ROOT, \"clean_224\")\n",
        "\n",
        "os.makedirs(RAW_ROOT, exist_ok=True)\n",
        "os.makedirs(DIR_32, exist_ok=True)\n",
        "os.makedirs(DIR_224, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Environment ready. Working directory: {BASE_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LW6Qqhjp1WRa"
      },
      "outputs": [],
      "source": [
        "def execute_crawling_and_processing(num_classes=10, images_per_class=20):\n",
        "    print(f\"\\n>>> [2/5] Randomly selecting {num_classes} CIFAR-100 classes...\")\n",
        "    temp_set = datasets.CIFAR100(root='./temp_meta', train=False, download=True)\n",
        "    cifar_classes = temp_set.classes\n",
        "\n",
        "    selected_keywords = random.sample(cifar_classes, num_classes)\n",
        "    print(f\"Selected targets: {selected_keywords}\")\n",
        "\n",
        "    for keyword in selected_keywords:\n",
        "        print(f\"Crawling: {keyword}...\")\n",
        "        save_dir = os.path.join(RAW_ROOT, keyword)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        crawler = BingImageCrawler(\n",
        "            downloader_threads=4,\n",
        "            storage={\"root_dir\": save_dir},\n",
        "            log_level=\"ERROR\"\n",
        "        )\n",
        "        crawler.crawl(keyword=f\"{keyword} photo\", max_num=images_per_class)\n",
        "\n",
        "    print(\"\\n>>> [3/5] Generating dual-version dataset (32px & 224px)...\")\n",
        "    valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "    count = 0\n",
        "\n",
        "    for class_name in os.listdir(RAW_ROOT):\n",
        "        class_dir = os.path.join(RAW_ROOT, class_name)\n",
        "        if not os.path.isdir(class_dir): continue\n",
        "\n",
        "        os.makedirs(os.path.join(DIR_32, class_name), exist_ok=True)\n",
        "        os.makedirs(os.path.join(DIR_224, class_name), exist_ok=True)\n",
        "\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if not any(filename.lower().endswith(ext) for ext in valid_exts): continue\n",
        "\n",
        "            try:\n",
        "                with Image.open(os.path.join(class_dir, filename)) as img:\n",
        "                    img = img.convert(\"RGB\")\n",
        "                    unique_name = f\"{class_name}_{uuid.uuid4().hex[:8]}.jpg\"\n",
        "\n",
        "                    img.resize((32, 32), Image.LANCZOS).save(\n",
        "                        os.path.join(DIR_32, class_name, unique_name), \"JPEG\", quality=95\n",
        "                    )\n",
        "\n",
        "                    img.resize((224, 224), Image.BICUBIC).save(\n",
        "                        os.path.join(DIR_224, class_name, unique_name), \"JPEG\", quality=95\n",
        "                    )\n",
        "                    count += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    print(f\"Processing completed! Generated {count} pairs of test images.\")\n",
        "    shutil.rmtree(RAW_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SXlivQyI1ZlU"
      },
      "outputs": [],
      "source": [
        "class ExactIndexDataset(Dataset):\n",
        "    def __init__(self, root_dir, cifar_classes, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        if not os.path.exists(root_dir): return\n",
        "\n",
        "        for class_name in os.listdir(root_dir):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_path) and class_name in cifar_classes:\n",
        "                real_index = cifar_classes.index(class_name)\n",
        "                for img in os.listdir(class_path):\n",
        "                    if img.endswith('.jpg'):\n",
        "                        self.samples.append((os.path.join(class_path, img), real_index))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "def get_universal_loader(model_type, batch_size=32):\n",
        "    if model_type == 'cifar_32':\n",
        "        mean, std = [0.507, 0.487, 0.441], [0.267, 0.256, 0.276]\n",
        "        data_path = DIR_32\n",
        "    elif model_type == 'imagenet_224':\n",
        "        mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "        data_path = DIR_224\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    full_classes = datasets.CIFAR100(root='./temp_meta', train=False).classes\n",
        "    dataset = ExactIndexDataset(data_path, full_classes, transform)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"Warning: No data found in {model_type} path! Please run crawling step first.\")\n",
        "        return None, full_classes\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False), full_classes\n",
        "\n",
        "def evaluate_model(model, dataloader, classes):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            _, predicted = torch.max(model(inputs), 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    report = classification_report(\n",
        "        all_targets, all_preds,\n",
        "        labels=list(range(100)), target_names=classes,\n",
        "        output_dict=True, zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"Subset Accuracy: {accuracy_score(all_targets, all_preds):.2%}\")\n",
        "    print(f\"{'Class':<15} | {'Prec.':<8} | {'Recall':<8} | {'Count'}\")\n",
        "    print(\"-\" * 45)\n",
        "    for name in classes:\n",
        "        if name in report and report[name]['support'] > 0:\n",
        "            r = report[name]\n",
        "            print(f\"{name:<15} | {r['precision']:.1%} | {r['recall']:.1%} | {int(r['support'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9GItrWo1cgF",
        "outputId": "476ebe6f-7d56-4ac3-91fd-a8b68201937b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> [2/5] Randomly selecting 10 CIFAR-100 classes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:14<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected targets: ['can', 'hamster', 'kangaroo', 'possum', 'lion', 'mountain', 'orchid', 'shark', 'maple_tree', 'mushroom']\n",
            "Crawling: can...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/863873832/photo/close-up-view-of-can.jpg\n",
            "ERROR:downloader:Response status code 404, file https://i0.wp.com/onlearn.es/wp-content/uploads/2023/03/can-grammar.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/697218800/photo/an-image-of-can.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: hamster...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/animal-mammal-hamster-rodent-fauna-guinea-pig-nuts-whiskers-vertebrate-gerbil-muroidea-goldhamster-1066386.jpg\n",
            "ERROR:downloader:Response status code 403, file https://p0.pikist.com/photos/686/588/cute-small-portrait-goldhamster-medium-hamster-pet-sweet-eat-food.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/mammal-hamster-rodent-fauna-cage-whiskers-vertebrate-button-eyes-gerbil-hamsters-muroidea-1045303.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/56/e9/mastomys_mice_rodents_cute_close_society_nager_fur-1093236.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/mouse-animal-cute-isolated-pet-studio-mammal-hamster-rodent-fauna-whiskers-rodents-vertebrate-muroidea-muridae-541621.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: kangaroo...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/wildlife-zoo-mammal-fauna-kangaroo-wallaby-vertebrate-marsupial-auodyssey-macropodidae-220207.jpg\n",
            "ERROR:downloader:Response status code 403, file https://p0.pikist.com/photos/690/910/wallaby-rednecked-wallaby-australia-queensland-marsupial-wild-kangaroo.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/wildlife-zoo-mammal-fauna-kangaroo-vertebrate-marsupial-safari-macropodidae-outdoor-recreation-106735.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-wildlife-mammal-fauna-kangaroo-wallaby-australia-vertebrate-marsupial-white-tailed-deer-macropodidae-1046849.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/wildlife-wild-mammal-fauna-kangaroo-wallaby-australia-gazelle-vertebrate-queensland-marsupial-vicuna-white-tailed-deer-pronghorn-macropodidae-rednecked-wallaby-guanaco-musk-deer-940969.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/tree-nature-grass-animal-cute-wildlife-wild-zoo-fur-mammal-fauna-kangaroo-wallaby-outdoors-branches-vertebrate-marsupial-white-tailed-deer-macropodidae-1176238.jpg\n",
            "ERROR:downloader:Response status code 404, file http://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Kangur.rudy.drs.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/3d/59/wallaby_kangaroo_animal_mammal_nature_australian_wildlife_wild-495686.jpg\n",
            "ERROR:downloader:Response status code 403, file https://p0.pikist.com/photos/857/959/wallaby-rednecked-wallaby-australia-queensland-marsupial-wild-kangaroo.jpg\n",
            "ERROR:downloader:Response status code 403, file https://p0.pikist.com/photos/424/438/kangaroo-wallaby-australia-animal.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/5c/58/wallaby_marsupial_animal_australia_red_neck_wallaby_wild_native_queensland-1094881.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/5e/99/animal_kangaroo_desert_outback_australia_wildlife_fauna_summer-731052.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/23/62/wallaby_rednecked_wallaby_female_australia_queensland_marsupial_wild_kangaroo-604314.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/5d/b6/kangaroo_zoo_tennessee_nashville_tn_animal_nature_mammal-1290341.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/wildlife-zoo-mammal-fauna-kangaroo-wallaby-australia-animals-vertebrate-marsupial-macropodidae-musk-deer-905724.jpg\n",
            "ERROR:downloader:Response status code 403, file https://p0.pikist.com/photos/870/726/wallabies-kangaroo-rednecked-wallaby-joey-mother-baby-two-pouch-australia.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/ee/60/kangaroo_wet_australia-1394893.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/69/51/kangaroo_joey_baby_wallaby_australia_marsupial_animal_hopping-1050611.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: possum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.rd.com/wp-content/uploads/2021/04/GettyImages-748566367.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.rd.com/wp-content/uploads/2021/04/GettyImages-139677758-scaled.jpg\n",
            "ERROR:downloader:Response status code 403, file http://humanedecisions.com/wp-content/uploads/2020/02/opossum-3933041_1920.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.rd.com/wp-content/uploads/2021/04/GettyImages-508348219-scaled.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: lion...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Exception caught when downloading file http://tolweb.org/tree/ToLimages/01008lion.400a.jpg, error: HTTPConnectionPool(host='tolweb.org', port=80): Max retries exceeded with url: /tree/ToLimages/01008lion.400a.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7829af0f1df0>, 'Connection to tolweb.org timed out. (connect timeout=5)')), remaining retry times: 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: mountain...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-rock-walking-mountain-snow-winter-sky-adventure-view-mountain-range-panorama-alpine-ridge-summit-mountain-peak-mountaineering-sports-mountains-alps-switzerland-appenzell-cirque-landform-mountain-pass-geographical-feature-mountainous-landforms-switzerland-s-ntis-santis-1203402.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/74/7e/dolomites_mountains_italy_alpine_south_tyrol_unesco_world_heritage_alpine_panorama_clouds-1385025.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/50/f9/kicking_horse_canadian_rockies_british_columbia_hiking_trail_mountain_top_yoho-609956.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/1b/e7/alta_badia_dolomites_mountains_south_tyrol_alpine_italy_unesco_world_heritage_alpine_panorama-1386063.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/85/3e/dolomites_mountains_italy_south_tyrol_alpine_hiking_unesco_world_heritage_alpine_panorama-1394950.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/f0/0d/dolomites_mountains_south_tyrol_alpine_italy_unesco_world_heritage_hiking_rubble_field-1388509.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/a6/a4/val_frisal_valley_brigels_river_hiking_water_nature_bank-950599.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: orchid...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://c.stocksy.com/a/clD000/z9/52924.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: shark...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://i1.pickpik.com/photos/94/786/386/shark-fish-eye-animal-thumb.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/1a/05/shark_teeth_underwater_sea_predator_jaw-942109.jpg\n",
            "ERROR:downloader:Response status code 403, file https://img1.whatsthatfish.com/unsafe/810x540/image.whatsthatfish.com/original/mEynI79zMe.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/c6/0a/shark_ocean_sea_water_animal_predator_nature_life-1334532.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/biology-blue-fish-shark-marine-vertebrate-marine-biology-requiem-shark-carcharhiniformes-tiger-shark-cartilaginous-fish-565928.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling: maple_tree...\n",
            "Crawling: mushroom...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-flower-autumn-mushroom-close-forest-floor-fungus-mushrooms-toxic-agaric-bolete-fungal-species-forest-mushroom-lamellar-matsutake-matryoshka-forest-plant-symbol-of-good-luck-screen-fungus-edible-mushroom-medicinal-mushroom-agaricaceae-penny-bun-603884.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-meadow-prairie-flower-gift-red-autumn-mushroom-point-flora-fungus-spotted-fly-agaric-toxic-amanita-agaric-bolete-amanita-muscaria-muscaria-red-fly-agaric-mushroom-macro-photography-edible-mushroom-medicinal-mushroom-penny-bun-amanitaceae-1157240.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-flower-autumn-botany-mushroom-flora-fauna-fungus-mushrooms-woodland-agaric-bolete-agaricus-matsutake-oyster-mushroom-edible-mushroom-medicinal-mushroom-agaricomycetes-agaricaceae-champignon-mushroom-penny-bun-pleurotus-eryngii-449717.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/92/9b/forest_fungi_fungus_mushrooms_nature-1181460.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/c3/5c/fly_agaric_lucky_guy_amanita_muscaria_fungal_species_red_firs_mushroom_autumn-1215748.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-autumn-mushroom-fungus-fly-agaric-agaric-bolete-agaricus-edible-mushroom-medicinal-mushroom-agaricaceae-penny-bun-1261754.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/67/11/mushrooms_tree_fungus_forest_brown_lamellar_b_schelig_group_pile-850709.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/eb/ae/mushroom_amanita_muscaria_fly_agaric_toadstool_cap_fungus_nature_fungi-1373582.jpg\n",
            "ERROR:downloader:Response status code 403, file https://get.pxhere.com/photo/nature-forest-leaf-autumn-soil-mushroom-flora-season-fauna-leaves-fungus-mushrooms-woodland-agaric-bolete-agaricus-macro-photography-bovist-oyster-mushroom-edible-mushroom-medicinal-mushroom-agaricomycetes-agaricaceae-champignon-mushroom-penny-bun-1135290.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.thedailyworld.com/wp-content/uploads/2024/06/36606047_web1_M1_ADW20240613_Muscaria-Mushroom-Gummies-Teaser.jpg\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> [3/5] Generating dual-version dataset (32px & 224px)...\n",
            "Processing completed! Generated 146 pairs of test images.\n"
          ]
        }
      ],
      "source": [
        "# Fetch & Pre-processing data\n",
        "execute_crawling_and_processing(num_classes=10, images_per_class=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CC-6OJ91ipO",
        "outputId": "4d6453fd-e4ae-4197-c7ce-c23d602f44a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing 32x32 model (CIFAR-100 Scratch)...\n",
            "Subset Accuracy: 38.54%\n",
            "Class           | Prec.    | Recall   | Count\n",
            "---------------------------------------------\n",
            "bed             | 100.0% | 30.0% | 10\n",
            "can             | 100.0% | 46.7% | 15\n",
            "flatfish        | 100.0% | 6.7% | 15\n",
            "girl            | 100.0% | 33.3% | 15\n",
            "hamster         | 100.0% | 26.7% | 15\n",
            "kangaroo        | 100.0% | 9.1% | 11\n",
            "lion            | 84.6% | 73.3% | 15\n",
            "lobster         | 66.7% | 40.0% | 15\n",
            "maple_tree      | 0.0% | 0.0% | 15\n",
            "mountain        | 100.0% | 53.3% | 15\n",
            "mouse           | 50.0% | 6.7% | 15\n",
            "mushroom        | 100.0% | 66.7% | 15\n",
            "orchid          | 81.8% | 60.0% | 15\n",
            "plate           | 100.0% | 40.0% | 15\n",
            "possum          | 85.7% | 40.0% | 15\n",
            "rabbit          | 100.0% | 13.3% | 15\n",
            "rose            | 100.0% | 60.0% | 15\n",
            "shark           | 100.0% | 40.0% | 15\n",
            "skyscraper      | 100.0% | 46.7% | 15\n",
            "tank            | 90.0% | 75.0% | 12\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting 32x32 model (CIFAR-100 Scratch)...\")\n",
        "\n",
        "loader_32, classes = get_universal_loader(model_type='cifar_32')\n",
        "\n",
        "if loader_32:\n",
        "    evaluate_model(net1, loader_32, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYun_ucS1lrK",
        "outputId": "faa291c5-c701-4805-dd5a-8776d248a288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing 224x224 model (ImageNet Transfer)...\n",
            "Subset Accuracy: 56.94%\n",
            "Class           | Prec.    | Recall   | Count\n",
            "---------------------------------------------\n",
            "bed             | 100.0% | 40.0% | 10\n",
            "can             | 100.0% | 33.3% | 15\n",
            "flatfish        | 100.0% | 20.0% | 15\n",
            "girl            | 100.0% | 6.7% | 15\n",
            "hamster         | 100.0% | 26.7% | 15\n",
            "kangaroo        | 100.0% | 81.8% | 11\n",
            "lion            | 100.0% | 80.0% | 15\n",
            "lobster         | 90.9% | 66.7% | 15\n",
            "maple_tree      | 0.0% | 0.0% | 15\n",
            "mountain        | 100.0% | 60.0% | 15\n",
            "mouse           | 20.0% | 13.3% | 15\n",
            "mushroom        | 100.0% | 73.3% | 15\n",
            "orchid          | 77.8% | 93.3% | 15\n",
            "plate           | 100.0% | 53.3% | 15\n",
            "possum          | 100.0% | 100.0% | 15\n",
            "rabbit          | 86.7% | 86.7% | 15\n",
            "rose            | 100.0% | 86.7% | 15\n",
            "shark           | 100.0% | 73.3% | 15\n",
            "skyscraper      | 100.0% | 66.7% | 15\n",
            "tank            | 100.0% | 83.3% | 12\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTesting 224x224 model (ImageNet Transfer)...\")\n",
        "\n",
        "loader_224, classes = get_universal_loader(model_type='imagenet_224')\n",
        "\n",
        "if loader_224:\n",
        "    evaluate_model(net, loader_224, classes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
